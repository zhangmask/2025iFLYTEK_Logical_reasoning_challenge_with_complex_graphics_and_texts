# å¤æ‚å›¾æ–‡é€»è¾‘æ¨ç†æŒ‘æˆ˜èµ› - å®æ–½æŒ‡å—ä¸ä»£ç ç¤ºä¾‹

## 1. ç¯å¢ƒé…ç½®

### 1.1 ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install streamlit pandas numpy scikit-learn
pip install websocket-client requests pillow
pip install transformers torch
pip install matplotlib seaborn plotly
```

### 1.2 é¡¹ç›®ç»“æ„

```
å¤æ‚å›¾æ–‡çš„é€»è¾‘æ¨ç†æŒ‘æˆ˜èµ›/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â”œâ”€â”€ sample_submit.csv
â”‚   â””â”€â”€ å›¾åƒæ•°æ®é›†/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_client.py          # è®¯é£APIå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ data_processor.py      # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ model_trainer.py       # æ¨¡å‹è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ inference_engine.py    # æ¨ç†å¼•æ“
â”‚   â””â”€â”€ utils.py              # å·¥å…·å‡½æ•°
â”œâ”€â”€ models/                   # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ outputs/                  # è¾“å‡ºç»“æœ
â”œâ”€â”€ logs/                     # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ app.py                    # Streamlitä¸»åº”ç”¨
â””â”€â”€ config.py                 # é…ç½®æ–‡ä»¶
```

## 2. æ ¸å¿ƒä»£ç å®ç°

### 2.1 è®¯é£APIå®¢æˆ·ç«¯ (api\_client.py)

```python
import websocket
import json
import base64
import hmac
import hashlib
from datetime import datetime
from time import mktime
from urllib.parse import urlencode, urlparse
from wsgiref.handlers import format_date_time
import threading
import time

class XunfeiImageAPI:
    def __init__(self, app_id, api_key, api_secret):
        self.app_id = app_id
        self.api_key = api_key
        self.api_secret = api_secret
        self.base_url = "wss://spark-api.cn-huabei-1.xf-yun.com/v2.1/image"
        self.response_text = ""
        self.response_complete = False
        
    def create_auth_url(self):
        """åˆ›å»ºè®¤è¯URL"""
        host = urlparse(self.base_url).netloc
        path = urlparse(self.base_url).path
        
        # ç”ŸæˆRFC1123æ ¼å¼çš„æ—¶é—´æˆ³
        now = datetime.now()
        date = format_date_time(mktime(now.timetuple()))
        
        # æ‹¼æ¥å­—ç¬¦ä¸²
        signature_origin = f"host: {host}\n"
        signature_origin += f"date: {date}\n"
        signature_origin += f"GET {path} HTTP/1.1"
        
        # HMAC-SHA256åŠ å¯†
        signature_sha = hmac.new(
            self.api_secret.encode('utf-8'),
            signature_origin.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        signature_sha_base64 = base64.b64encode(signature_sha).decode(encoding='utf-8')
        
        authorization_origin = f'api_key="{self.api_key}", algorithm="hmac-sha256", headers="host date request-line", signature="{signature_sha_base64}"'
        authorization = base64.b64encode(authorization_origin.encode('utf-8')).decode(encoding='utf-8')
        
        # æ„å»ºURLå‚æ•°
        params = {
            "authorization": authorization,
            "date": date,
            "host": host
        }
        
        return self.base_url + '?' + urlencode(params)
    
    def on_message(self, ws, message):
        """å¤„ç†WebSocketæ¶ˆæ¯"""
        try:
            data = json.loads(message)
            code = data['header']['code']
            
            if code != 0:
                print(f"APIè°ƒç”¨é”™è¯¯: {data['header']['message']}")
                self.response_complete = True
                return
                
            choices = data["payload"]["choices"]
            status = choices["status"]
            content = choices["text"][0]["content"]
            
            self.response_text += content
            
            if status == 2:  # ç»“æŸ
                self.response_complete = True
                ws.close()
                
        except Exception as e:
            print(f"æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")
            self.response_complete = True
    
    def on_error(self, ws, error):
        """å¤„ç†WebSocketé”™è¯¯"""
        print(f"WebSocketé”™è¯¯: {error}")
        self.response_complete = True
    
    def on_close(self, ws, close_status_code, close_msg):
        """WebSocketå…³é—­"""
        self.response_complete = True
    
    def on_open(self, ws):
        """WebSocketè¿æ¥å»ºç«‹"""
        def run():
            # å‘é€è¯·æ±‚æ•°æ®
            ws.send(json.dumps(self.request_data))
        
        threading.Thread(target=run).start()
    
    def understand_image(self, image_path, question="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯è§çš„æ–‡å­—ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ç­‰ä¿¡æ¯ã€‚"):
        """ç†è§£å›¾ç‰‡å†…å®¹"""
        try:
            # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
            with open(image_path, 'rb') as f:
                image_data = f.read()
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            self.request_data = {
                "header": {
                    "app_id": self.app_id,
                    "uid": "user123"
                },
                "parameter": {
                    "chat": {
                        "domain": "image",
                        "temperature": 0.5,
                        "max_tokens": 2048
                    }
                },
                "payload": {
                    "message": {
                        "text": [
                            {
                                "role": "user",
                                "content": question
                            }
                        ],
                        "image_url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            }
            
            # é‡ç½®å“åº”çŠ¶æ€
            self.response_text = ""
            self.response_complete = False
            
            # åˆ›å»ºWebSocketè¿æ¥
            auth_url = self.create_auth_url()
            ws = websocket.WebSocketApp(
                auth_url,
                on_message=self.on_message,
                on_error=self.on_error,
                on_close=self.on_close,
                on_open=self.on_open
            )
            
            # å¯åŠ¨è¿æ¥
            ws.run_forever()
            
            # ç­‰å¾…å“åº”å®Œæˆ
            timeout = 30  # 30ç§’è¶…æ—¶
            start_time = time.time()
            while not self.response_complete and (time.time() - start_time) < timeout:
                time.sleep(0.1)
            
            if not self.response_complete:
                raise Exception("APIè°ƒç”¨è¶…æ—¶")
                
            return self.response_text
            
        except Exception as e:
            print(f"å›¾ç‰‡ç†è§£å¤±è´¥: {e}")
            return None
```

### 2.2 æ•°æ®å¤„ç†æ¨¡å— (data\_processor.py)

```python
import pandas as pd
import json
import re
from pathlib import Path
from api_client import XunfeiImageAPI
import time

class DataProcessor:
    def __init__(self, api_client):
        self.api_client = api_client
        self.processed_descriptions = {}
        
    def load_data(self, data_path):
        """åŠ è½½æ•°æ®é›†"""
        train_df = pd.read_csv(data_path / 'train.csv')
        test_df = pd.read_csv(data_path / 'test.csv')
        return train_df, test_df
    
    def process_image_batch(self, df, image_base_path, batch_size=5):
        """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
        results = []
        
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            
            for idx, row in batch.iterrows():
                try:
                    image_path = image_base_path / row['image']
                    
                    if str(image_path) in self.processed_descriptions:
                        description = self.processed_descriptions[str(image_path)]
                    else:
                        # è°ƒç”¨APIç†è§£å›¾ç‰‡
                        description = self.api_client.understand_image(
                            str(image_path),
                            "è¯·è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ–‡å­—ã€äººç‰©ã€ç‰©ä½“ã€åœºæ™¯ã€æ•°å­—ã€è¡¨æ ¼ç­‰ä¿¡æ¯ï¼Œä»¥ä¾¿è¿›è¡Œé€»è¾‘æ¨ç†ã€‚"
                        )
                        
                        if description:
                            self.processed_descriptions[str(image_path)] = description
                        
                        # APIè°ƒç”¨é—´éš”
                        time.sleep(1)
                    
                    results.append({
                        'id': row['id'],
                        'image': row['image'],
                        'question': row['question'],
                        'answer': row.get('answer', ''),
                        'description': description
                    })
                    
                    print(f"å¤„ç†å®Œæˆ: {idx+1}/{len(df)} - {row['image']}")
                    
                except Exception as e:
                    print(f"å¤„ç†å¤±è´¥ {row['image']}: {e}")
                    results.append({
                        'id': row['id'],
                        'image': row['image'],
                        'question': row['question'],
                        'answer': row.get('answer', ''),
                        'description': None
                    })
        
        return pd.DataFrame(results)
    
    def extract_knowledge_graph(self, description):
        """ä»æè¿°ä¸­æå–çŸ¥è¯†å›¾è°±è¾¹é›†"""
        if not description:
            return []
        
        edges = []
        
        # ç®€å•çš„å®ä½“å…³ç³»æå–
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯
        
        # æå–æ•°å­—å…³ç³»
        numbers = re.findall(r'\d+(?:\.\d+)?', description)
        for num in numbers:
            edges.append(('å›¾ç‰‡', 'åŒ…å«æ•°å­—', num))
        
        # æå–é¢œè‰²å…³ç³»
        colors = re.findall(r'(çº¢è‰²|è“è‰²|ç»¿è‰²|é»„è‰²|é»‘è‰²|ç™½è‰²|ç°è‰²|ç´«è‰²|æ©™è‰²|ç²‰è‰²)', description)
        for color in colors:
            edges.append(('å›¾ç‰‡', 'åŒ…å«é¢œè‰²', color))
        
        # æå–äººç‰©å…³ç³»
        if 'äºº' in description or 'ç”·' in description or 'å¥³' in description:
            edges.append(('å›¾ç‰‡', 'åŒ…å«', 'äººç‰©'))
        
        # æå–æ–‡å­—å…³ç³»
        if 'æ–‡å­—' in description or 'å­—' in description or 'æ–‡æœ¬' in description:
            edges.append(('å›¾ç‰‡', 'åŒ…å«', 'æ–‡å­—'))
        
        # æå–è¡¨æ ¼å…³ç³»
        if 'è¡¨æ ¼' in description or 'è¡¨' in description:
            edges.append(('å›¾ç‰‡', 'åŒ…å«', 'è¡¨æ ¼'))
        
        return edges
    
    def create_features(self, df):
        """åˆ›å»ºç‰¹å¾"""
        features = []
        
        for _, row in df.iterrows():
            # åŸºç¡€ç‰¹å¾
            feature_dict = {
                'id': row['id'],
                'question_length': len(row['question']),
                'description_length': len(row['description']) if row['description'] else 0,
                'has_description': 1 if row['description'] else 0
            }
            
            # é—®é¢˜ç‰¹å¾
            question = row['question']
            feature_dict.update({
                'question_has_number': 1 if re.search(r'\d+', question) else 0,
                'question_has_time': 1 if any(word in question for word in ['æ—¶é—´', 'æ—¥æœŸ', 'ä½•æ—¶', 'ä»€ä¹ˆæ—¶å€™']) else 0,
                'question_has_who': 1 if any(word in question for word in ['è°', 'å“ªä¸ª', 'å“ªä½']) else 0,
                'question_has_why': 1 if any(word in question for word in ['ä¸ºä½•', 'ä¸ºä»€ä¹ˆ', 'åŸå› ']) else 0,
                'question_has_how': 1 if any(word in question for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ€æ ·']) else 0
            })
            
            # æè¿°ç‰¹å¾
            if row['description']:
                description = row['description']
                feature_dict.update({
                    'desc_has_number': 1 if re.search(r'\d+', description) else 0,
                    'desc_has_time': 1 if any(word in description for word in ['æ—¶é—´', 'æ—¥æœŸ', 'å¹´', 'æœˆ', 'æ—¥']) else 0,
                    'desc_has_person': 1 if any(word in description for word in ['äºº', 'ç”·', 'å¥³', 'äººç‰©']) else 0,
                    'desc_has_text': 1 if any(word in description for word in ['æ–‡å­—', 'å­—', 'æ–‡æœ¬']) else 0
                })
            else:
                feature_dict.update({
                    'desc_has_number': 0,
                    'desc_has_time': 0,
                    'desc_has_person': 0,
                    'desc_has_text': 0
                })
            
            # çŸ¥è¯†å›¾è°±ç‰¹å¾
            edges = self.extract_knowledge_graph(row['description'])
            feature_dict['kg_edge_count'] = len(edges)
            
            features.append(feature_dict)
        
        return pd.DataFrame(features)
    
    def save_processed_data(self, df, output_path):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
```

### 2.3 æ¨¡å‹è®­ç»ƒæ¨¡å— (model\_trainer.py)

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import joblib
import json
from pathlib import Path

class ModelTrainer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        
    def prepare_training_data(self, processed_df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [
            'question_length', 'description_length', 'has_description',
            'question_has_number', 'question_has_time', 'question_has_who',
            'question_has_why', 'question_has_how', 'desc_has_number',
            'desc_has_time', 'desc_has_person', 'desc_has_text', 'kg_edge_count'
        ]
        
        self.feature_columns = feature_cols
        
        # æå–ç‰¹å¾å’Œç›®æ ‡
        X = processed_df[feature_cols].fillna(0)
        
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç›®æ ‡å˜é‡
        # ç”±äºè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºç­”æ¡ˆé•¿åº¦ç­‰ç‰¹å¾åˆ›å»ºå›å½’ç›®æ ‡
        y = processed_df['answer'].str.len().fillna(0)
        
        return X, y
    
    def train_models(self, X, y, test_size=0.2, random_state=42):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        # åˆ†å‰²æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        self.scalers['main'] = scaler
        
        # å®šä¹‰æ¨¡å‹
        models_to_train = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=random_state),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state),
            'linear_regression': LinearRegression()
        }
        
        results = {}
        
        for name, model in models_to_train.items():
            print(f"è®­ç»ƒæ¨¡å‹: {name}")
            
            # è®­ç»ƒæ¨¡å‹
            if name == 'linear_regression':
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_val_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
            
            # è¯„ä¼°æ¨¡å‹
            mse = mean_squared_error(y_val, y_pred)
            mae = mean_absolute_error(y_val, y_pred)
            
            results[name] = {
                'model': model,
                'mse': mse,
                'mae': mae,
                'predictions': y_pred
            }
            
            self.models[name] = model
            
            print(f"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}")
        
        return results
    
    def save_models(self, output_dir):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        for name, model in self.models.items():
            model_path = output_dir / f"{name}_model.pkl"
            joblib.dump(model, model_path)
            print(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        for name, scaler in self.scalers.items():
            scaler_path = output_dir / f"{name}_scaler.pkl"
            joblib.dump(scaler, scaler_path)
            print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
        
        # ä¿å­˜ç‰¹å¾åˆ—ä¿¡æ¯
        feature_info = {
            'feature_columns': self.feature_columns
        }
        
        with open(output_dir / 'feature_info.json', 'w', encoding='utf-8') as f:
            json.dump(feature_info, f, ensure_ascii=False, indent=2)
    
    def load_models(self, model_dir):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_dir = Path(model_dir)
        
        # åŠ è½½ç‰¹å¾ä¿¡æ¯
        with open(model_dir / 'feature_info.json', 'r', encoding='utf-8') as f:
            feature_info = json.load(f)
        
        self.feature_columns = feature_info['feature_columns']
        
        # åŠ è½½æ¨¡å‹
        for model_file in model_dir.glob('*_model.pkl'):
            model_name = model_file.stem.replace('_model', '')
            self.models[model_name] = joblib.load(model_file)
            print(f"æ¨¡å‹å·²åŠ è½½: {model_name}")
        
        # åŠ è½½æ ‡å‡†åŒ–å™¨
        for scaler_file in model_dir.glob('*_scaler.pkl'):
            scaler_name = scaler_file.stem.replace('_scaler', '')
            self.scalers[scaler_name] = joblib.load(scaler_file)
            print(f"æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {scaler_name}")
```

### 2.4 æ¨ç†å¼•æ“ (inference\_engine.py)

```python
import pandas as pd
import numpy as np
from pathlib import Path
import re
from difflib import SequenceMatcher

class InferenceEngine:
    def __init__(self, model_trainer, data_processor):
        self.model_trainer = model_trainer
        self.data_processor = data_processor
        
    def calculate_jaccard_similarity(self, str1, str2):
        """è®¡ç®—å­—ç¬¦çº§Jaccardç›¸ä¼¼åº¦"""
        if not str1 or not str2:
            return 0.0
        
        set1 = set(str1)
        set2 = set(str2)
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def generate_answer_from_description(self, question, description):
        """åŸºäºæè¿°å’Œé—®é¢˜ç”Ÿæˆç­”æ¡ˆ"""
        if not description:
            return "æ— æ³•ä»å›¾ç‰‡ä¸­è·å–ç›¸å…³ä¿¡æ¯ã€‚"
        
        # ç®€å•çš„è§„åˆ™åŸºç¡€ç­”æ¡ˆç”Ÿæˆ
        question_lower = question.lower()
        description_lower = description.lower()
        
        # æ—¶é—´ç›¸å…³é—®é¢˜
        if any(word in question for word in ['ä½•æ—¶', 'ä»€ä¹ˆæ—¶å€™', 'æ—¶é—´', 'æ—¥æœŸ']):
            # æå–æ—¶é—´ä¿¡æ¯
            time_patterns = [
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{1,2}:\d{1,2})',
            ]
            
            for pattern in time_patterns:
                matches = re.findall(pattern, description)
                if matches:
                    return matches[0]
        
        # äººç‰©ç›¸å…³é—®é¢˜
        if any(word in question for word in ['è°', 'å“ªä½', 'å“ªä¸ªäºº']):
            # æå–äººå
            person_patterns = [
                r'([\u4e00-\u9fa5]{2,4}(?:å…ˆç”Ÿ|å¥³å£«|æ•™ç»ƒ|ç»ç†|ä¸»å¸­|æ€»è£|CEO))',
                r'([A-Za-z]+\s[A-Za-z]+)',
            ]
            
            for pattern in person_patterns:
                matches = re.findall(pattern, description)
                if matches:
                    return matches[0]
        
        # æ•°é‡ç›¸å…³é—®é¢˜
        if any(word in question for word in ['å¤šå°‘', 'å‡ ä¸ª', 'æ•°é‡']):
            numbers = re.findall(r'\d+(?:\.\d+)?', description)
            if numbers:
                return numbers[0]
        
        # åŸå› ç›¸å…³é—®é¢˜
        if any(word in question for word in ['ä¸ºä½•', 'ä¸ºä»€ä¹ˆ', 'åŸå› ']):
            # å¯»æ‰¾å› æœå…³ç³»è¯æ±‡
            causal_words = ['å› ä¸º', 'ç”±äº', 'å¯¼è‡´', 'é€ æˆ', 'å¼•èµ·']
            for word in causal_words:
                if word in description:
                    # æå–åŸå› éƒ¨åˆ†
                    parts = description.split(word)
                    if len(parts) > 1:
                        return parts[1][:100]  # è¿”å›å‰100ä¸ªå­—ç¬¦
        
        # æ–¹å¼ç›¸å…³é—®é¢˜
        if any(word in question for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ€æ ·']):
            # å¯»æ‰¾æ–¹æ³•æè¿°
            method_words = ['é€šè¿‡', 'é‡‡ç”¨', 'ä½¿ç”¨', 'åˆ©ç”¨', 'æ–¹æ³•']
            for word in method_words:
                if word in description:
                    parts = description.split(word)
                    if len(parts) > 1:
                        return parts[1][:100]
        
        # é»˜è®¤è¿”å›æè¿°çš„å…³é”®éƒ¨åˆ†
        sentences = description.split('ã€‚')
        if sentences:
            # è¿”å›æœ€ç›¸å…³çš„å¥å­
            best_sentence = ""
            best_score = 0
            
            for sentence in sentences:
                if len(sentence) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                    score = self.calculate_text_similarity(question, sentence)
                    if score > best_score:
                        best_score = score
                        best_sentence = sentence
            
            return best_sentence if best_sentence else sentences[0][:100]
        
        return description[:100]  # è¿”å›å‰100ä¸ªå­—ç¬¦
    
    def calculate_text_similarity(self, text1, text2):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def predict_batch(self, test_df):
        """æ‰¹é‡é¢„æµ‹"""
        predictions = []
        
        for _, row in test_df.iterrows():
            try:
                # ç”Ÿæˆç­”æ¡ˆ
                answer = self.generate_answer_from_description(
                    row['question'], 
                    row['description']
                )
                
                predictions.append({
                    'id': row['id'],
                    'answer': answer
                })
                
            except Exception as e:
                print(f"é¢„æµ‹å¤±è´¥ ID {row['id']}: {e}")
                predictions.append({
                    'id': row['id'],
                    'answer': "é¢„æµ‹å¤±è´¥"
                })
        
        return pd.DataFrame(predictions)
    
    def evaluate_predictions(self, predictions_df, ground_truth_df):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        results = []
        
        for _, pred_row in predictions_df.iterrows():
            truth_row = ground_truth_df[ground_truth_df['id'] == pred_row['id']]
            
            if not truth_row.empty:
                true_answer = truth_row.iloc[0]['answer']
                pred_answer = pred_row['answer']
                
                jaccard_sim = self.calculate_jaccard_similarity(true_answer, pred_answer)
                
                results.append({
                    'id': pred_row['id'],
                    'predicted': pred_answer,
                    'ground_truth': true_answer,
                    'jaccard_similarity': jaccard_sim
                })
        
        results_df = pd.DataFrame(results)
        avg_jaccard = results_df['jaccard_similarity'].mean()
        
        print(f"å¹³å‡Jaccardç›¸ä¼¼åº¦: {avg_jaccard:.4f}")
        
        return results_df, avg_jaccard
```

## 3. ä¸»åº”ç”¨ (app.py)

```python
import streamlit as st
import pandas as pd
from pathlib import Path
import sys

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append('src')

from api_client import XunfeiImageAPI
from data_processor import DataProcessor
from model_trainer import ModelTrainer
from inference_engine import InferenceEngine

# é…ç½®ä¿¡æ¯
APP_ID = "5e0dd074"
API_KEY = "49e7eacaf518a39697317a21692a0cde"
API_SECRET = "NzA3YWQ1YjczNjFmZDgzMmMwOTk4Yjhj"

def main():
    st.set_page_config(
        page_title="å¤æ‚å›¾æ–‡é€»è¾‘æ¨ç†æŒ‘æˆ˜èµ›",
        page_icon="ğŸ§ ",
        layout="wide"
    )
    
    st.title("ğŸ§  å¤æ‚å›¾æ–‡é€»è¾‘æ¨ç†æŒ‘æˆ˜èµ›è§£å†³æ–¹æ¡ˆ")
    
    # ä¾§è¾¹æ å¯¼èˆª
    page = st.sidebar.selectbox(
        "é€‰æ‹©åŠŸèƒ½é¡µé¢",
        ["é¡¹ç›®æ¦‚è§ˆ", "æ•°æ®å¤„ç†", "æ¨¡å‹è®­ç»ƒ", "æ¨ç†é¢„æµ‹", "ç³»ç»Ÿé…ç½®"]
    )
    
    if page == "é¡¹ç›®æ¦‚è§ˆ":
        show_overview()
    elif page == "æ•°æ®å¤„ç†":
        show_data_processing()
    elif page == "æ¨¡å‹è®­ç»ƒ":
        show_model_training()
    elif page == "æ¨ç†é¢„æµ‹":
        show_inference()
    elif page == "ç³»ç»Ÿé…ç½®":
        show_configuration()

def show_overview():
    st.header("ğŸ“Š é¡¹ç›®æ¦‚è§ˆ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("è®­ç»ƒæ•°æ®é‡", "100æ¡")
    with col2:
        st.metric("æµ‹è¯•æ•°æ®é‡", "384æ¡")
    with col3:
        st.metric("è¯„ä¼°æŒ‡æ ‡", "Jaccardç›¸ä¼¼åº¦")
    
    st.subheader("ğŸ¯ é¡¹ç›®ç›®æ ‡")
    st.write("""
    - ä½¿ç”¨è®¯é£æ˜Ÿç«å›¾ç‰‡ç†è§£APIå¤„ç†å›¾åƒå†…å®¹
    - å°†å›¾ç‰‡ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬æè¿°
    - åŸºäºé—®é¢˜å’Œå›¾ç‰‡æè¿°è¿›è¡Œé€»è¾‘æ¨ç†
    - ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆå¹¶ä¼˜åŒ–ç›¸ä¼¼åº¦å¾—åˆ†
    """)
    
    st.subheader("ğŸ”„ å¤„ç†æµç¨‹")
    st.write("""
    1. **æ•°æ®é¢„å¤„ç†**: è°ƒç”¨APIè·å–å›¾ç‰‡æè¿°
    2. **ç‰¹å¾å·¥ç¨‹**: æå–æ–‡æœ¬ç‰¹å¾å’ŒçŸ¥è¯†å›¾è°±
    3. **æ¨¡å‹è®­ç»ƒ**: è®­ç»ƒå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹
    4. **æ¨ç†é¢„æµ‹**: ç”Ÿæˆæµ‹è¯•é›†ç­”æ¡ˆ
    5. **ç»“æœè¯„ä¼°**: è®¡ç®—Jaccardç›¸ä¼¼åº¦
    """)

def show_data_processing():
    st.header("ğŸ”„ æ•°æ®å¤„ç†")
    
    # APIå®¢æˆ·ç«¯åˆå§‹åŒ–
    if 'api_client' not in st.session_state:
        st.session_state.api_client = XunfeiImageAPI(APP_ID, API_KEY, API_SECRET)
        st.session_state.data_processor = DataProcessor(st.session_state.api_client)
    
    st.subheader("ğŸ“ æ•°æ®åŠ è½½")
    
    data_path = st.text_input("æ•°æ®è·¯å¾„", value="./")
    
    if st.button("åŠ è½½æ•°æ®"):
        try:
            train_df, test_df = st.session_state.data_processor.load_data(Path(data_path))
            st.session_state.train_df = train_df
            st.session_state.test_df = test_df
            
            st.success(f"è®­ç»ƒæ•°æ®: {len(train_df)}æ¡, æµ‹è¯•æ•°æ®: {len(test_df)}æ¡")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("è®­ç»ƒæ•°æ®é¢„è§ˆ")
            st.dataframe(train_df.head())
            
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    st.subheader("ğŸ–¼ï¸ å›¾ç‰‡ç†è§£å¤„ç†")
    
    if 'train_df' in st.session_state:
        col1, col2 = st.columns(2)
        
        with col1:
            process_train = st.checkbox("å¤„ç†è®­ç»ƒé›†")
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=1, max_value=10, value=5)
        
        with col2:
            process_test = st.checkbox("å¤„ç†æµ‹è¯•é›†")
            image_path = st.text_input("å›¾ç‰‡è·¯å¾„", value="./å›¾åƒæ•°æ®é›†")
        
        if st.button("å¼€å§‹å¤„ç†"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                if process_train:
                    status_text.text("æ­£åœ¨å¤„ç†è®­ç»ƒé›†...")
                    processed_train = st.session_state.data_processor.process_image_batch(
                        st.session_state.train_df, 
                        Path(image_path), 
                        batch_size
                    )
                    st.session_state.processed_train = processed_train
                    progress_bar.progress(0.5)
                
                if process_test:
                    status_text.text("æ­£åœ¨å¤„ç†æµ‹è¯•é›†...")
                    processed_test = st.session_state.data_processor.process_image_batch(
                        st.session_state.test_df, 
                        Path(image_path), 
                        batch_size
                    )
                    st.session_state.processed_test = processed_test
                    progress_bar.progress(1.0)
                
                status_text.text("å¤„ç†å®Œæˆ!")
                st.success("å›¾ç‰‡ç†è§£å¤„ç†å®Œæˆ")
                
            except Exception as e:
                st.error(f"å¤„ç†å¤±è´¥: {e}")

def show_model_training():
    st.header("ğŸ¤– æ¨¡å‹è®­ç»ƒ")
    
    if 'processed_train' not in st.session_state:
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®å¤„ç†")
        return
    
    # æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–
    if 'model_trainer' not in st.session_state:
        st.session_state.model_trainer = ModelTrainer()
    
    st.subheader("ğŸ“Š ç‰¹å¾å·¥ç¨‹")
    
    if st.button("åˆ›å»ºç‰¹å¾"):
        try:
            features_df = st.session_state.data_processor.create_features(
                st.session_state.processed_train
            )
            st.session_state.features_df = features_df
            
            st.success("ç‰¹å¾åˆ›å»ºå®Œæˆ")
            st.dataframe(features_df.head())
            
        except Exception as e:
            st.error(f"ç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
    
    st.subheader("ğŸ¯ æ¨¡å‹è®­ç»ƒ")
    
    if 'features_df' in st.session_state:
        col1, col2 = st.columns(2)
        
        with col1:
            test_size = st.slider("éªŒè¯é›†æ¯”ä¾‹", 0.1, 0.5, 0.2)
        with col2:
            random_state = st.number_input("éšæœºç§å­", value=42)
        
        if st.button("å¼€å§‹è®­ç»ƒ"):
            try:
                # å‡†å¤‡è®­ç»ƒæ•°æ®
                X, y = st.session_state.model_trainer.prepare_training_data(
                    st.session_state.processed_train
                )
                
                # è®­ç»ƒæ¨¡å‹
                results = st.session_state.model_trainer.train_models(
                    X, y, test_size, random_state
                )
                
                st.session_state.training_results = results
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("è®­ç»ƒç»“æœ")
                for name, result in results.items():
                    st.write(f"**{name}**")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("MSE", f"{result['mse']:.4f}")
                    with col2:
                        st.metric("MAE", f"{result['mae']:.4f}")
                
                # ä¿å­˜æ¨¡å‹
                if st.button("ä¿å­˜æ¨¡å‹"):
                    st.session_state.model_trainer.save_models("./models")
                    st.success("æ¨¡å‹å·²ä¿å­˜")
                    
            except Exception as e:
                st.error(f"è®­ç»ƒå¤±è´¥: {e}")

def show_inference():
    st.header("ğŸ”® æ¨ç†é¢„æµ‹")
    
    if 'processed_test' not in st.session_state:
        st.warning("è¯·å…ˆå®Œæˆæµ‹è¯•æ•°æ®å¤„ç†")
        return
    
    # æ¨ç†å¼•æ“åˆå§‹åŒ–
    if 'inference_engine' not in st.session_state:
        if 'model_trainer' in st.session_state and 'data_processor' in st.session_state:
            st.session_state.inference_engine = InferenceEngine(
                st.session_state.model_trainer,
                st.session_state.data_processor
            )
    
    st.subheader("ğŸ“ æ‰¹é‡é¢„æµ‹")
    
    if st.button("å¼€å§‹é¢„æµ‹"):
        try:
            predictions = st.session_state.inference_engine.predict_batch(
                st.session_state.processed_test
            )
            
            st.session_state.predictions = predictions
            
            st.success(f"é¢„æµ‹å®Œæˆï¼Œå…±{len(predictions)}æ¡ç»“æœ")
            st.dataframe(predictions.head(10))
            
        except Exception as e:
            st.error(f"é¢„æµ‹å¤±è´¥: {e}")
    
    st.subheader("ğŸ’¾ ç»“æœå¯¼å‡º")
    
    if 'predictions' in st.session_state:
        if st.button("å¯¼å‡ºç»“æœ"):
            try:
                output_path = "./outputs/submission.csv"
                Path("./outputs").mkdir(exist_ok=True)
                
                st.session_state.predictions.to_csv(
                    output_path, index=False, encoding='utf-8'
                )
                
                st.success(f"ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")
                
                # æä¾›ä¸‹è½½é“¾æ¥
                with open(output_path, 'rb') as f:
                    st.download_button(
                        label="ä¸‹è½½æäº¤æ–‡ä»¶",
                        data=f.read(),
                        file_name="submission.csv",
                        mime="text/csv"
                    )
                    
            except Exception as e:
                st.error(f"å¯¼å‡ºå¤±è´¥: {e}")

def show_configuration():
    st.header("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    st.subheader("ğŸ”‘ APIé…ç½®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        app_id = st.text_input("APP ID", value=APP_ID)
        api_key = st.text_input("API Key", value=API_KEY, type="password")
    
    with col2:
        api_secret = st.text_input("API Secret", value=API_SECRET, type="password")
        api_url = st.text_input("API URL", value="wss://spark-api.cn-huabei-1.xf-yun.com/v2.1/image")
    
    if st.button("æµ‹è¯•APIè¿æ¥"):
        try:
            test_client = XunfeiImageAPI(app_id, api_key, api_secret)
            st.success("APIé…ç½®æœ‰æ•ˆ")
        except Exception as e:
            st.error(f"APIé…ç½®é”™è¯¯: {e}")
    
    st.subheader("ğŸ“‹ ç³»ç»ŸçŠ¶æ€")
    
    status_data = {
        "ç»„ä»¶": ["APIå®¢æˆ·ç«¯", "æ•°æ®å¤„ç†å™¨", "æ¨¡å‹è®­ç»ƒå™¨", "æ¨ç†å¼•æ“"],
        "çŠ¶æ€": [
            "âœ… å·²åˆå§‹åŒ–" if 'api_client' in st.session_state else "âŒ æœªåˆå§‹åŒ–",
            "âœ… å·²åˆå§‹åŒ–" if 'data_processor' in st.session_state else "âŒ æœªåˆå§‹åŒ–",
            "âœ… å·²åˆå§‹åŒ–" if 'model_trainer' in st.session_state else "âŒ æœªåˆå§‹åŒ–",
            "âœ… å·²åˆå§‹åŒ–" if 'inference_engine' in st.session_state else "âŒ æœªåˆå§‹åŒ–"
        ]
    }
    
    st.table(pd.DataFrame(status_data))

if __name__ == "__main__":
    main()
```

## 4. è¿è¡ŒæŒ‡å—

### 4.1 å¯åŠ¨åº”ç”¨

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd å¤æ‚å›¾æ–‡çš„é€»è¾‘æ¨ç†æŒ‘æˆ˜èµ›

# å¯åŠ¨Streamlitåº”ç”¨
streamlit run app.py
```

### 4.2 ä½¿ç”¨æµç¨‹

1. **æ•°æ®å¤„ç†**: ä¸Šä¼ æ•°æ®é›†ï¼Œè°ƒç”¨è®¯é£APIå¤„ç†å›¾ç‰‡
2. **æ¨¡å‹è®­ç»ƒ**: åˆ›å»ºç‰¹å¾ï¼Œè®­ç»ƒå¤šç§æ¨¡å‹
3. **æ¨ç†é¢„æµ‹**: å¯¹æµ‹è¯•é›†è¿›è¡Œæ‰¹é‡é¢„æµ‹
4. **ç»“æœå¯¼å‡º**: ç”Ÿæˆæäº¤æ–‡ä»¶

### 4.3 æ³¨æ„äº‹é¡¹

* APIè°ƒç”¨æœ‰é¢‘ç‡é™åˆ¶ï¼Œå»ºè®®è®¾ç½®é€‚å½“çš„å»¶æ—¶

* å›¾ç‰‡æ–‡ä»¶è·¯å¾„éœ€è¦æ­£ç¡®é…ç½®

* æ¨¡å‹è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´

* ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´ä¿å­˜å¤„ç†ç»“æœ

## 5. æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **APIè°ƒç”¨ä¼˜åŒ–**: å®ç°ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è°ƒç”¨
2. **å¹¶è¡Œå¤„ç†**: ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†å›¾ç‰‡æ‰¹æ¬¡
3. **ç‰¹å¾å·¥ç¨‹**: æ·»åŠ æ›´å¤šæœ‰æ•ˆç‰¹å¾
4. **æ¨¡å‹é›†æˆ**: ä½¿ç”¨é›†æˆå­¦ä¹ æé«˜é¢„æµ‹å‡†ç¡®æ€§
5. **ç­”æ¡ˆç”Ÿæˆ**: æ”¹è¿›è§„åˆ™åŸºç¡€çš„ç­”æ¡ˆç”Ÿæˆé€»è¾‘

