# 复杂图文逻辑推理挑战赛 - 实施指南与代码示例

## 1. 环境配置

### 1.1 依赖安装

```bash
# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 安装核心依赖
pip install streamlit pandas numpy scikit-learn
pip install websocket-client requests pillow
pip install transformers torch
pip install matplotlib seaborn plotly
```

### 1.2 项目结构

```
复杂图文的逻辑推理挑战赛/
├── data/
│   ├── train.csv
│   ├── test.csv
│   ├── sample_submit.csv
│   └── 图像数据集/
├── src/
│   ├── api_client.py          # 讯飞API客户端
│   ├── data_processor.py      # 数据处理模块
│   ├── model_trainer.py       # 模型训练模块
│   ├── inference_engine.py    # 推理引擎
│   └── utils.py              # 工具函数
├── models/                   # 训练好的模型
├── outputs/                  # 输出结果
├── logs/                     # 日志文件
├── app.py                    # Streamlit主应用
└── config.py                 # 配置文件
```

## 2. 核心代码实现

### 2.1 讯飞API客户端 (api\_client.py)

```python
import websocket
import json
import base64
import hmac
import hashlib
from datetime import datetime
from time import mktime
from urllib.parse import urlencode, urlparse
from wsgiref.handlers import format_date_time
import threading
import time

class XunfeiImageAPI:
    def __init__(self, app_id, api_key, api_secret):
        self.app_id = app_id
        self.api_key = api_key
        self.api_secret = api_secret
        self.base_url = "wss://spark-api.cn-huabei-1.xf-yun.com/v2.1/image"
        self.response_text = ""
        self.response_complete = False
        
    def create_auth_url(self):
        """创建认证URL"""
        host = urlparse(self.base_url).netloc
        path = urlparse(self.base_url).path
        
        # 生成RFC1123格式的时间戳
        now = datetime.now()
        date = format_date_time(mktime(now.timetuple()))
        
        # 拼接字符串
        signature_origin = f"host: {host}\n"
        signature_origin += f"date: {date}\n"
        signature_origin += f"GET {path} HTTP/1.1"
        
        # HMAC-SHA256加密
        signature_sha = hmac.new(
            self.api_secret.encode('utf-8'),
            signature_origin.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        signature_sha_base64 = base64.b64encode(signature_sha).decode(encoding='utf-8')
        
        authorization_origin = f'api_key="{self.api_key}", algorithm="hmac-sha256", headers="host date request-line", signature="{signature_sha_base64}"'
        authorization = base64.b64encode(authorization_origin.encode('utf-8')).decode(encoding='utf-8')
        
        # 构建URL参数
        params = {
            "authorization": authorization,
            "date": date,
            "host": host
        }
        
        return self.base_url + '?' + urlencode(params)
    
    def on_message(self, ws, message):
        """处理WebSocket消息"""
        try:
            data = json.loads(message)
            code = data['header']['code']
            
            if code != 0:
                print(f"API调用错误: {data['header']['message']}")
                self.response_complete = True
                return
                
            choices = data["payload"]["choices"]
            status = choices["status"]
            content = choices["text"][0]["content"]
            
            self.response_text += content
            
            if status == 2:  # 结束
                self.response_complete = True
                ws.close()
                
        except Exception as e:
            print(f"消息处理错误: {e}")
            self.response_complete = True
    
    def on_error(self, ws, error):
        """处理WebSocket错误"""
        print(f"WebSocket错误: {error}")
        self.response_complete = True
    
    def on_close(self, ws, close_status_code, close_msg):
        """WebSocket关闭"""
        self.response_complete = True
    
    def on_open(self, ws):
        """WebSocket连接建立"""
        def run():
            # 发送请求数据
            ws.send(json.dumps(self.request_data))
        
        threading.Thread(target=run).start()
    
    def understand_image(self, image_path, question="请详细描述这张图片的内容，包括所有可见的文字、人物、物体、场景等信息。"):
        """理解图片内容"""
        try:
            # 读取并编码图片
            with open(image_path, 'rb') as f:
                image_data = f.read()
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # 构建请求数据
            self.request_data = {
                "header": {
                    "app_id": self.app_id,
                    "uid": "user123"
                },
                "parameter": {
                    "chat": {
                        "domain": "image",
                        "temperature": 0.5,
                        "max_tokens": 2048
                    }
                },
                "payload": {
                    "message": {
                        "text": [
                            {
                                "role": "user",
                                "content": question
                            }
                        ],
                        "image_url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            }
            
            # 重置响应状态
            self.response_text = ""
            self.response_complete = False
            
            # 创建WebSocket连接
            auth_url = self.create_auth_url()
            ws = websocket.WebSocketApp(
                auth_url,
                on_message=self.on_message,
                on_error=self.on_error,
                on_close=self.on_close,
                on_open=self.on_open
            )
            
            # 启动连接
            ws.run_forever()
            
            # 等待响应完成
            timeout = 30  # 30秒超时
            start_time = time.time()
            while not self.response_complete and (time.time() - start_time) < timeout:
                time.sleep(0.1)
            
            if not self.response_complete:
                raise Exception("API调用超时")
                
            return self.response_text
            
        except Exception as e:
            print(f"图片理解失败: {e}")
            return None
```

### 2.2 数据处理模块 (data\_processor.py)

```python
import pandas as pd
import json
import re
from pathlib import Path
from api_client import XunfeiImageAPI
import time

class DataProcessor:
    def __init__(self, api_client):
        self.api_client = api_client
        self.processed_descriptions = {}
        
    def load_data(self, data_path):
        """加载数据集"""
        train_df = pd.read_csv(data_path / 'train.csv')
        test_df = pd.read_csv(data_path / 'test.csv')
        return train_df, test_df
    
    def process_image_batch(self, df, image_base_path, batch_size=5):
        """批量处理图片"""
        results = []
        
        for i in range(0, len(df), batch_size):
            batch = df.iloc[i:i+batch_size]
            
            for idx, row in batch.iterrows():
                try:
                    image_path = image_base_path / row['image']
                    
                    if str(image_path) in self.processed_descriptions:
                        description = self.processed_descriptions[str(image_path)]
                    else:
                        # 调用API理解图片
                        description = self.api_client.understand_image(
                            str(image_path),
                            "请详细描述图片内容，包括所有文字、人物、物体、场景、数字、表格等信息，以便进行逻辑推理。"
                        )
                        
                        if description:
                            self.processed_descriptions[str(image_path)] = description
                        
                        # API调用间隔
                        time.sleep(1)
                    
                    results.append({
                        'id': row['id'],
                        'image': row['image'],
                        'question': row['question'],
                        'answer': row.get('answer', ''),
                        'description': description
                    })
                    
                    print(f"处理完成: {idx+1}/{len(df)} - {row['image']}")
                    
                except Exception as e:
                    print(f"处理失败 {row['image']}: {e}")
                    results.append({
                        'id': row['id'],
                        'image': row['image'],
                        'question': row['question'],
                        'answer': row.get('answer', ''),
                        'description': None
                    })
        
        return pd.DataFrame(results)
    
    def extract_knowledge_graph(self, description):
        """从描述中提取知识图谱边集"""
        if not description:
            return []
        
        edges = []
        
        # 简单的实体关系提取
        # 这里可以使用更复杂的NLP技术
        
        # 提取数字关系
        numbers = re.findall(r'\d+(?:\.\d+)?', description)
        for num in numbers:
            edges.append(('图片', '包含数字', num))
        
        # 提取颜色关系
        colors = re.findall(r'(红色|蓝色|绿色|黄色|黑色|白色|灰色|紫色|橙色|粉色)', description)
        for color in colors:
            edges.append(('图片', '包含颜色', color))
        
        # 提取人物关系
        if '人' in description or '男' in description or '女' in description:
            edges.append(('图片', '包含', '人物'))
        
        # 提取文字关系
        if '文字' in description or '字' in description or '文本' in description:
            edges.append(('图片', '包含', '文字'))
        
        # 提取表格关系
        if '表格' in description or '表' in description:
            edges.append(('图片', '包含', '表格'))
        
        return edges
    
    def create_features(self, df):
        """创建特征"""
        features = []
        
        for _, row in df.iterrows():
            # 基础特征
            feature_dict = {
                'id': row['id'],
                'question_length': len(row['question']),
                'description_length': len(row['description']) if row['description'] else 0,
                'has_description': 1 if row['description'] else 0
            }
            
            # 问题特征
            question = row['question']
            feature_dict.update({
                'question_has_number': 1 if re.search(r'\d+', question) else 0,
                'question_has_time': 1 if any(word in question for word in ['时间', '日期', '何时', '什么时候']) else 0,
                'question_has_who': 1 if any(word in question for word in ['谁', '哪个', '哪位']) else 0,
                'question_has_why': 1 if any(word in question for word in ['为何', '为什么', '原因']) else 0,
                'question_has_how': 1 if any(word in question for word in ['如何', '怎么', '怎样']) else 0
            })
            
            # 描述特征
            if row['description']:
                description = row['description']
                feature_dict.update({
                    'desc_has_number': 1 if re.search(r'\d+', description) else 0,
                    'desc_has_time': 1 if any(word in description for word in ['时间', '日期', '年', '月', '日']) else 0,
                    'desc_has_person': 1 if any(word in description for word in ['人', '男', '女', '人物']) else 0,
                    'desc_has_text': 1 if any(word in description for word in ['文字', '字', '文本']) else 0
                })
            else:
                feature_dict.update({
                    'desc_has_number': 0,
                    'desc_has_time': 0,
                    'desc_has_person': 0,
                    'desc_has_text': 0
                })
            
            # 知识图谱特征
            edges = self.extract_knowledge_graph(row['description'])
            feature_dict['kg_edge_count'] = len(edges)
            
            features.append(feature_dict)
        
        return pd.DataFrame(features)
    
    def save_processed_data(self, df, output_path):
        """保存处理后的数据"""
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"数据已保存到: {output_path}")
```

### 2.3 模型训练模块 (model\_trainer.py)

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import joblib
import json
from pathlib import Path

class ModelTrainer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        
    def prepare_training_data(self, processed_df):
        """准备训练数据"""
        # 选择特征列
        feature_cols = [
            'question_length', 'description_length', 'has_description',
            'question_has_number', 'question_has_time', 'question_has_who',
            'question_has_why', 'question_has_how', 'desc_has_number',
            'desc_has_time', 'desc_has_person', 'desc_has_text', 'kg_edge_count'
        ]
        
        self.feature_columns = feature_cols
        
        # 提取特征和目标
        X = processed_df[feature_cols].fillna(0)
        
        # 这里我们需要创建一个目标变量
        # 由于这是一个文本生成任务，我们可以基于答案长度等特征创建回归目标
        y = processed_df['answer'].str.len().fillna(0)
        
        return X, y
    
    def train_models(self, X, y, test_size=0.2, random_state=42):
        """训练多个模型"""
        # 分割数据
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # 标准化特征
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        
        self.scalers['main'] = scaler
        
        # 定义模型
        models_to_train = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=random_state),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state),
            'linear_regression': LinearRegression()
        }
        
        results = {}
        
        for name, model in models_to_train.items():
            print(f"训练模型: {name}")
            
            # 训练模型
            if name == 'linear_regression':
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_val_scaled)
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
            
            # 评估模型
            mse = mean_squared_error(y_val, y_pred)
            mae = mean_absolute_error(y_val, y_pred)
            
            results[name] = {
                'model': model,
                'mse': mse,
                'mae': mae,
                'predictions': y_pred
            }
            
            self.models[name] = model
            
            print(f"{name} - MSE: {mse:.4f}, MAE: {mae:.4f}")
        
        return results
    
    def save_models(self, output_dir):
        """保存训练好的模型"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # 保存模型
        for name, model in self.models.items():
            model_path = output_dir / f"{name}_model.pkl"
            joblib.dump(model, model_path)
            print(f"模型已保存: {model_path}")
        
        # 保存标准化器
        for name, scaler in self.scalers.items():
            scaler_path = output_dir / f"{name}_scaler.pkl"
            joblib.dump(scaler, scaler_path)
            print(f"标准化器已保存: {scaler_path}")
        
        # 保存特征列信息
        feature_info = {
            'feature_columns': self.feature_columns
        }
        
        with open(output_dir / 'feature_info.json', 'w', encoding='utf-8') as f:
            json.dump(feature_info, f, ensure_ascii=False, indent=2)
    
    def load_models(self, model_dir):
        """加载训练好的模型"""
        model_dir = Path(model_dir)
        
        # 加载特征信息
        with open(model_dir / 'feature_info.json', 'r', encoding='utf-8') as f:
            feature_info = json.load(f)
        
        self.feature_columns = feature_info['feature_columns']
        
        # 加载模型
        for model_file in model_dir.glob('*_model.pkl'):
            model_name = model_file.stem.replace('_model', '')
            self.models[model_name] = joblib.load(model_file)
            print(f"模型已加载: {model_name}")
        
        # 加载标准化器
        for scaler_file in model_dir.glob('*_scaler.pkl'):
            scaler_name = scaler_file.stem.replace('_scaler', '')
            self.scalers[scaler_name] = joblib.load(scaler_file)
            print(f"标准化器已加载: {scaler_name}")
```

### 2.4 推理引擎 (inference\_engine.py)

```python
import pandas as pd
import numpy as np
from pathlib import Path
import re
from difflib import SequenceMatcher

class InferenceEngine:
    def __init__(self, model_trainer, data_processor):
        self.model_trainer = model_trainer
        self.data_processor = data_processor
        
    def calculate_jaccard_similarity(self, str1, str2):
        """计算字符级Jaccard相似度"""
        if not str1 or not str2:
            return 0.0
        
        set1 = set(str1)
        set2 = set(str2)
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def generate_answer_from_description(self, question, description):
        """基于描述和问题生成答案"""
        if not description:
            return "无法从图片中获取相关信息。"
        
        # 简单的规则基础答案生成
        question_lower = question.lower()
        description_lower = description.lower()
        
        # 时间相关问题
        if any(word in question for word in ['何时', '什么时候', '时间', '日期']):
            # 提取时间信息
            time_patterns = [
                r'(\d{4}年\d{1,2}月\d{1,2}日)',
                r'(\d{4}-\d{1,2}-\d{1,2})',
                r'(\d{1,2}月\d{1,2}日)',
                r'(\d{1,2}:\d{1,2})',
            ]
            
            for pattern in time_patterns:
                matches = re.findall(pattern, description)
                if matches:
                    return matches[0]
        
        # 人物相关问题
        if any(word in question for word in ['谁', '哪位', '哪个人']):
            # 提取人名
            person_patterns = [
                r'([\u4e00-\u9fa5]{2,4}(?:先生|女士|教练|经理|主席|总裁|CEO))',
                r'([A-Za-z]+\s[A-Za-z]+)',
            ]
            
            for pattern in person_patterns:
                matches = re.findall(pattern, description)
                if matches:
                    return matches[0]
        
        # 数量相关问题
        if any(word in question for word in ['多少', '几个', '数量']):
            numbers = re.findall(r'\d+(?:\.\d+)?', description)
            if numbers:
                return numbers[0]
        
        # 原因相关问题
        if any(word in question for word in ['为何', '为什么', '原因']):
            # 寻找因果关系词汇
            causal_words = ['因为', '由于', '导致', '造成', '引起']
            for word in causal_words:
                if word in description:
                    # 提取原因部分
                    parts = description.split(word)
                    if len(parts) > 1:
                        return parts[1][:100]  # 返回前100个字符
        
        # 方式相关问题
        if any(word in question for word in ['如何', '怎么', '怎样']):
            # 寻找方法描述
            method_words = ['通过', '采用', '使用', '利用', '方法']
            for word in method_words:
                if word in description:
                    parts = description.split(word)
                    if len(parts) > 1:
                        return parts[1][:100]
        
        # 默认返回描述的关键部分
        sentences = description.split('。')
        if sentences:
            # 返回最相关的句子
            best_sentence = ""
            best_score = 0
            
            for sentence in sentences:
                if len(sentence) > 10:  # 过滤太短的句子
                    score = self.calculate_text_similarity(question, sentence)
                    if score > best_score:
                        best_score = score
                        best_sentence = sentence
            
            return best_sentence if best_sentence else sentences[0][:100]
        
        return description[:100]  # 返回前100个字符
    
    def calculate_text_similarity(self, text1, text2):
        """计算文本相似度"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def predict_batch(self, test_df):
        """批量预测"""
        predictions = []
        
        for _, row in test_df.iterrows():
            try:
                # 生成答案
                answer = self.generate_answer_from_description(
                    row['question'], 
                    row['description']
                )
                
                predictions.append({
                    'id': row['id'],
                    'answer': answer
                })
                
            except Exception as e:
                print(f"预测失败 ID {row['id']}: {e}")
                predictions.append({
                    'id': row['id'],
                    'answer': "预测失败"
                })
        
        return pd.DataFrame(predictions)
    
    def evaluate_predictions(self, predictions_df, ground_truth_df):
        """评估预测结果"""
        results = []
        
        for _, pred_row in predictions_df.iterrows():
            truth_row = ground_truth_df[ground_truth_df['id'] == pred_row['id']]
            
            if not truth_row.empty:
                true_answer = truth_row.iloc[0]['answer']
                pred_answer = pred_row['answer']
                
                jaccard_sim = self.calculate_jaccard_similarity(true_answer, pred_answer)
                
                results.append({
                    'id': pred_row['id'],
                    'predicted': pred_answer,
                    'ground_truth': true_answer,
                    'jaccard_similarity': jaccard_sim
                })
        
        results_df = pd.DataFrame(results)
        avg_jaccard = results_df['jaccard_similarity'].mean()
        
        print(f"平均Jaccard相似度: {avg_jaccard:.4f}")
        
        return results_df, avg_jaccard
```

## 3. 主应用 (app.py)

```python
import streamlit as st
import pandas as pd
from pathlib import Path
import sys

# 添加src目录到路径
sys.path.append('src')

from api_client import XunfeiImageAPI
from data_processor import DataProcessor
from model_trainer import ModelTrainer
from inference_engine import InferenceEngine

# 配置信息
APP_ID = "5e0dd074"
API_KEY = "49e7eacaf518a39697317a21692a0cde"
API_SECRET = "NzA3YWQ1YjczNjFmZDgzMmMwOTk4Yjhj"

def main():
    st.set_page_config(
        page_title="复杂图文逻辑推理挑战赛",
        page_icon="🧠",
        layout="wide"
    )
    
    st.title("🧠 复杂图文逻辑推理挑战赛解决方案")
    
    # 侧边栏导航
    page = st.sidebar.selectbox(
        "选择功能页面",
        ["项目概览", "数据处理", "模型训练", "推理预测", "系统配置"]
    )
    
    if page == "项目概览":
        show_overview()
    elif page == "数据处理":
        show_data_processing()
    elif page == "模型训练":
        show_model_training()
    elif page == "推理预测":
        show_inference()
    elif page == "系统配置":
        show_configuration()

def show_overview():
    st.header("📊 项目概览")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("训练数据量", "100条")
    with col2:
        st.metric("测试数据量", "384条")
    with col3:
        st.metric("评估指标", "Jaccard相似度")
    
    st.subheader("🎯 项目目标")
    st.write("""
    - 使用讯飞星火图片理解API处理图像内容
    - 将图片信息转换为结构化文本描述
    - 基于问题和图片描述进行逻辑推理
    - 生成准确的答案并优化相似度得分
    """)
    
    st.subheader("🔄 处理流程")
    st.write("""
    1. **数据预处理**: 调用API获取图片描述
    2. **特征工程**: 提取文本特征和知识图谱
    3. **模型训练**: 训练多种机器学习模型
    4. **推理预测**: 生成测试集答案
    5. **结果评估**: 计算Jaccard相似度
    """)

def show_data_processing():
    st.header("🔄 数据处理")
    
    # API客户端初始化
    if 'api_client' not in st.session_state:
        st.session_state.api_client = XunfeiImageAPI(APP_ID, API_KEY, API_SECRET)
        st.session_state.data_processor = DataProcessor(st.session_state.api_client)
    
    st.subheader("📁 数据加载")
    
    data_path = st.text_input("数据路径", value="./")
    
    if st.button("加载数据"):
        try:
            train_df, test_df = st.session_state.data_processor.load_data(Path(data_path))
            st.session_state.train_df = train_df
            st.session_state.test_df = test_df
            
            st.success(f"训练数据: {len(train_df)}条, 测试数据: {len(test_df)}条")
            
            # 显示数据预览
            st.subheader("训练数据预览")
            st.dataframe(train_df.head())
            
        except Exception as e:
            st.error(f"数据加载失败: {e}")
    
    st.subheader("🖼️ 图片理解处理")
    
    if 'train_df' in st.session_state:
        col1, col2 = st.columns(2)
        
        with col1:
            process_train = st.checkbox("处理训练集")
            batch_size = st.number_input("批次大小", min_value=1, max_value=10, value=5)
        
        with col2:
            process_test = st.checkbox("处理测试集")
            image_path = st.text_input("图片路径", value="./图像数据集")
        
        if st.button("开始处理"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                if process_train:
                    status_text.text("正在处理训练集...")
                    processed_train = st.session_state.data_processor.process_image_batch(
                        st.session_state.train_df, 
                        Path(image_path), 
                        batch_size
                    )
                    st.session_state.processed_train = processed_train
                    progress_bar.progress(0.5)
                
                if process_test:
                    status_text.text("正在处理测试集...")
                    processed_test = st.session_state.data_processor.process_image_batch(
                        st.session_state.test_df, 
                        Path(image_path), 
                        batch_size
                    )
                    st.session_state.processed_test = processed_test
                    progress_bar.progress(1.0)
                
                status_text.text("处理完成!")
                st.success("图片理解处理完成")
                
            except Exception as e:
                st.error(f"处理失败: {e}")

def show_model_training():
    st.header("🤖 模型训练")
    
    if 'processed_train' not in st.session_state:
        st.warning("请先完成数据处理")
        return
    
    # 模型训练器初始化
    if 'model_trainer' not in st.session_state:
        st.session_state.model_trainer = ModelTrainer()
    
    st.subheader("📊 特征工程")
    
    if st.button("创建特征"):
        try:
            features_df = st.session_state.data_processor.create_features(
                st.session_state.processed_train
            )
            st.session_state.features_df = features_df
            
            st.success("特征创建完成")
            st.dataframe(features_df.head())
            
        except Exception as e:
            st.error(f"特征创建失败: {e}")
    
    st.subheader("🎯 模型训练")
    
    if 'features_df' in st.session_state:
        col1, col2 = st.columns(2)
        
        with col1:
            test_size = st.slider("验证集比例", 0.1, 0.5, 0.2)
        with col2:
            random_state = st.number_input("随机种子", value=42)
        
        if st.button("开始训练"):
            try:
                # 准备训练数据
                X, y = st.session_state.model_trainer.prepare_training_data(
                    st.session_state.processed_train
                )
                
                # 训练模型
                results = st.session_state.model_trainer.train_models(
                    X, y, test_size, random_state
                )
                
                st.session_state.training_results = results
                
                # 显示结果
                st.subheader("训练结果")
                for name, result in results.items():
                    st.write(f"**{name}**")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("MSE", f"{result['mse']:.4f}")
                    with col2:
                        st.metric("MAE", f"{result['mae']:.4f}")
                
                # 保存模型
                if st.button("保存模型"):
                    st.session_state.model_trainer.save_models("./models")
                    st.success("模型已保存")
                    
            except Exception as e:
                st.error(f"训练失败: {e}")

def show_inference():
    st.header("🔮 推理预测")
    
    if 'processed_test' not in st.session_state:
        st.warning("请先完成测试数据处理")
        return
    
    # 推理引擎初始化
    if 'inference_engine' not in st.session_state:
        if 'model_trainer' in st.session_state and 'data_processor' in st.session_state:
            st.session_state.inference_engine = InferenceEngine(
                st.session_state.model_trainer,
                st.session_state.data_processor
            )
    
    st.subheader("📝 批量预测")
    
    if st.button("开始预测"):
        try:
            predictions = st.session_state.inference_engine.predict_batch(
                st.session_state.processed_test
            )
            
            st.session_state.predictions = predictions
            
            st.success(f"预测完成，共{len(predictions)}条结果")
            st.dataframe(predictions.head(10))
            
        except Exception as e:
            st.error(f"预测失败: {e}")
    
    st.subheader("💾 结果导出")
    
    if 'predictions' in st.session_state:
        if st.button("导出结果"):
            try:
                output_path = "./outputs/submission.csv"
                Path("./outputs").mkdir(exist_ok=True)
                
                st.session_state.predictions.to_csv(
                    output_path, index=False, encoding='utf-8'
                )
                
                st.success(f"结果已导出到: {output_path}")
                
                # 提供下载链接
                with open(output_path, 'rb') as f:
                    st.download_button(
                        label="下载提交文件",
                        data=f.read(),
                        file_name="submission.csv",
                        mime="text/csv"
                    )
                    
            except Exception as e:
                st.error(f"导出失败: {e}")

def show_configuration():
    st.header("⚙️ 系统配置")
    
    st.subheader("🔑 API配置")
    
    col1, col2 = st.columns(2)
    
    with col1:
        app_id = st.text_input("APP ID", value=APP_ID)
        api_key = st.text_input("API Key", value=API_KEY, type="password")
    
    with col2:
        api_secret = st.text_input("API Secret", value=API_SECRET, type="password")
        api_url = st.text_input("API URL", value="wss://spark-api.cn-huabei-1.xf-yun.com/v2.1/image")
    
    if st.button("测试API连接"):
        try:
            test_client = XunfeiImageAPI(app_id, api_key, api_secret)
            st.success("API配置有效")
        except Exception as e:
            st.error(f"API配置错误: {e}")
    
    st.subheader("📋 系统状态")
    
    status_data = {
        "组件": ["API客户端", "数据处理器", "模型训练器", "推理引擎"],
        "状态": [
            "✅ 已初始化" if 'api_client' in st.session_state else "❌ 未初始化",
            "✅ 已初始化" if 'data_processor' in st.session_state else "❌ 未初始化",
            "✅ 已初始化" if 'model_trainer' in st.session_state else "❌ 未初始化",
            "✅ 已初始化" if 'inference_engine' in st.session_state else "❌ 未初始化"
        ]
    }
    
    st.table(pd.DataFrame(status_data))

if __name__ == "__main__":
    main()
```

## 4. 运行指南

### 4.1 启动应用

```bash
# 进入项目目录
cd 复杂图文的逻辑推理挑战赛

# 启动Streamlit应用
streamlit run app.py
```

### 4.2 使用流程

1. **数据处理**: 上传数据集，调用讯飞API处理图片
2. **模型训练**: 创建特征，训练多种模型
3. **推理预测**: 对测试集进行批量预测
4. **结果导出**: 生成提交文件

### 4.3 注意事项

* API调用有频率限制，建议设置适当的延时

* 图片文件路径需要正确配置

* 模型训练可能需要较长时间

* 确保有足够的存储空间保存处理结果

## 5. 性能优化建议

1. **API调用优化**: 实现缓存机制，避免重复调用
2. **并行处理**: 使用多线程处理图片批次
3. **特征工程**: 添加更多有效特征
4. **模型集成**: 使用集成学习提高预测准确性
5. **答案生成**: 改进规则基础的答案生成逻辑

